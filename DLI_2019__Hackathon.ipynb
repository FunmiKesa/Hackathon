{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLI 2019_ Hackathon.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "tO1T8OVjztN5",
        "p-vCM1YBXmYb",
        "8jzNXvDsgAeq",
        "J-pu7_MLSmhP",
        "_SPWX35wdJlf",
        "HkCG_h2wiDI2",
        "syZ0UtrMA5WH",
        "9BdhcB6syrjL",
        "J4DJ2ixxfYp4",
        "UlrSyMQpoV9f"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FunmiKesa/Hackathon/blob/master/DLI_2019__Hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO1T8OVjztN5",
        "colab_type": "text"
      },
      "source": [
        "# DLI 2019 Hackathon\n",
        "\n",
        "**The theme** of the hackathon is wildlife conservation, and how AI, machine learning and data science can contribute. \n",
        "\n",
        "**The Dataset** will come from Zooniverse: https://www.zooniverse.org/projects/zooniverse/snapshot-serengeti.\n",
        "Based on Snapshot Serengeti from https://snapshotserengeti.org\n",
        "\n",
        "You can read more about [the data](https://www.nature.com/articles/sdata201526) and some initial research done under the \"Snapshot Serengeti\" project [here](https://arxiv.org/abs/1703.05830).\n",
        "\n",
        "It is copyright and licensed under Creative Commons Attribution 4.0 International License. \n",
        "https://creativecommons.org/licenses/by/4.0/legalcode\n",
        "\n",
        "The following modifications were made with permission from the authors:\n",
        "  * Reduced the dataset to only feature the ten most populous species.\n",
        "  * Prepared the data as TF Examples and saved in TF Records format to ease integration with TensorFlow.\n",
        "  \n",
        "\n",
        "**The dataset is split into:**\n",
        "\n",
        "  * **Training set:**   236795 examples\n",
        "  * **Validation set**:  13157 examples\n",
        "  * **Test set:**        13154 examples\n",
        "\n",
        "---  \n",
        "\n",
        "**The following features are available for each datapoint:**\n",
        "\n",
        "  * **Features**:\n",
        "    * '**Images**': Between 1-3 JPEG encoded photographs of each event.\n",
        "    * '**Embeddings**': A 128-dimensional feature representation of each image. Obtained from a [ResNet-50 (v1)](https://arxiv.org/abs/1512.03385) pretrained on ImageNet and fine-tuned on this data. This should make it easier for you to try some ML algorithms without working on raw image data.\n",
        "    * '**SiteID**': The site id of the camera.\n",
        "    * '**CaptureEventID**': A unique ID for the capture event.\n",
        "    * '**LocationX**': Location of the camera (x)\n",
        "    * '**LocationY**': Location of the camera (y)\n",
        "    * '**DateTime**': The timestamp the of the capture event.\n",
        "   \n",
        "\n",
        "\n",
        "  * **Lables**:\n",
        "    * '**NumSpecies**': Number of species identified.\n",
        "    * '**Count**': The number of animals of each species identified.\n",
        "    * '**Species**': The species present at the event.\n",
        "  \n",
        "\n",
        "**The following species are present**: \n",
        "  * buffalo\n",
        "  * elephant\n",
        "  * gazelleGrants\n",
        "  * gazelleThomsons\n",
        "  * giraffe\n",
        "  * guineaFowl\n",
        "  * hartebeest\n",
        "  * impala\n",
        "  * wildebeest\n",
        "  * zebra\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-vCM1YBXmYb"
      },
      "source": [
        "# First steps: Imports and data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJG0K_T7wifE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import functools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"TensorFlow executing eagerly: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jzNXvDsgAeq",
        "colab_type": "text"
      },
      "source": [
        "### First steps\n",
        "\n",
        "1.   Go to the folder \"Serengeti Hackathon\" shared with you\n",
        "2.   Right click the folder name and choose \"Add to My Drive\" from the drop-down menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqJx9lm0YC8U",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Global variables [RUN ME!]\n",
        "\n",
        "_TRAINING_SET_SIZE = 236795\n",
        "_VALIDATION_SET_SIZE = 13157\n",
        "_TEST_SET_SIZE = 13154\n",
        "\n",
        "# All possible anmial species present in the dataset\n",
        "_SPECIES = ['buffalo',\n",
        "            'elephant',\n",
        "            'gazelleGrants',\n",
        "            'gazelleThomsons',\n",
        "            'giraffe',\n",
        "            'guineaFowl',\n",
        "            'hartebeest',\n",
        "            'impala',\n",
        "            'wildebeest',\n",
        "            'zebra',\n",
        "            'none',  # No animals in the photograph\n",
        "]\n",
        "\n",
        "_SPECIES_TO_CLASS_ID = dict(zip(_SPECIES, range(len(_SPECIES))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtHwIzV2OuYs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Copy data from Google Cloud Storage to local disk\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "bucket_name = 'dli_indaba2019'\n",
        "train_filename = 'training_set_with_embeddings128.tfrecord'\n",
        "valid_filename = 'validation_set_with_embeddings128.tfrecord'\n",
        "test_filename = 'test_set_with_embeddings128.tfrecord'\n",
        "download_dir = '.'\n",
        "!gsutil cp gs://{bucket_name}/{train_filename} {download_dir}\n",
        "!gsutil cp gs://{bucket_name}/{valid_filename} {download_dir}\n",
        "!gsutil cp gs://{bucket_name}/{test_filename} {download_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usQ70N6nPVFA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load data\n",
        "training_set_path = \"training_set_with_embeddings128.tfrecord\"\n",
        "validation_set_path = \"validation_set_with_embeddings128.tfrecord\"\n",
        "test_set_path =  \"test_set_with_embeddings128.tfrecord\"\n",
        "\n",
        "# Create a `tf.data.Dataset` object from the recordio data\n",
        "training_dataset = tf.data.TFRecordDataset(training_set_path)\n",
        "validation_dataset = tf.data.TFRecordDataset(validation_set_path)\n",
        "test_dataset = tf.data.TFRecordDataset(test_set_path)\n",
        "\n",
        "# Shuffle data in the training set\n",
        "training_dataset = training_dataset.shuffle(10000)\n",
        "validation_dataset = validation_dataset\n",
        "test_dataset = test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-pu7_MLSmhP",
        "colab_type": "text"
      },
      "source": [
        "# Exploring the dataset\n",
        "\n",
        "Now that we've set up the tensorflow objects, let's look at some of the training\n",
        "datapoints to get a better feeling for the data we're dealing with.\n",
        "\n",
        "For this, we'll first define some plotting functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHclc9pV7rjk",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Adding some parsing & preprocessing utility functions\n",
        "\n",
        "def show_single_record(example):\n",
        "  \"\"\"Show images, species and their count in one record.\"\"\"\n",
        "  \n",
        "  # Decode all images associated with this record\n",
        "  im_raws = example.features.feature['Images'].bytes_list.value\n",
        "  im = [tf.image.decode_image(i) for i in im_raws]\n",
        "  \n",
        "  plt.figure(figsize=(10, 20))\n",
        "  plt.imshow(np.hstack(im))\n",
        "  labels = [example.features.feature[k].bytes_list.value \n",
        "            for k in [\"Species\",\"Count\"]]\n",
        "  \n",
        "  species = [s.decode('utf-8') for s in labels[0]]\n",
        "  counts = [l for l in labels[1]]\n",
        "  plt.title(f'Species: {species},  Counts: {counts}, Image shape: {im[0].shape}')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  \n",
        "  return len(im)\n",
        "\n",
        "def show_records(dataset, least_nb_labels=1, stop=5):\n",
        "  \"\"\"Show records with at least \"least_nb_labels\" labels and finish after 'stop' cases.\"\"\"\n",
        "  example = tf.train.Example()\n",
        "  nb_cases = 0\n",
        "  for raw_record in dataset:\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    species = example.features.feature['Species'].bytes_list.value\n",
        "    if len(species) >= least_nb_labels:\n",
        "      nb_cases += 1\n",
        "      show_single_record(example)\n",
        "      if nb_cases == stop: \n",
        "        return\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ngs8NfdeIx3r",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "And then look at some of photographs. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NlP0dAsIwu3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Plot some images\n",
        "\n",
        "how_many = 10 #@param\n",
        "\n",
        "show_records(test_dataset, stop=how_many)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjNvrc6aVHVx",
        "colab_type": "text"
      },
      "source": [
        "Some observations:\n",
        "\n",
        "* It turns out not all records have a constant number of images, some have 1, 2 or even 3.\n",
        "* Sometimes images are take at unsual angles, at night, in grayscale or under vastly different lighting conditions.\n",
        "* Animals appear in varying distances to the camera.\n",
        "* We can have multiple animals in a single photograph.\n",
        "* Fortunately, all images have the same size: 256 x 341 and always have 3 colour channels.\n",
        "* **What else did you notice?**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "290B7YBxQeb_",
        "colab_type": "text"
      },
      "source": [
        "The function `show_record_with_many_labels` has a `least_nb_labels` argument. What happens if we set this to a value higher than 1?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzXlfyCM0FH",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Plot images with `least_nb_labels` = 2\n",
        "\n",
        "how_many = 5 #@param\n",
        "\n",
        "show_records(\n",
        "    training_dataset, least_nb_labels=2, stop=how_many)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKfcqTKMNfMI",
        "colab_type": "text"
      },
      "source": [
        "Turns out some of photographs not only have **multiple animals** but also **multiple species** in them!\n",
        "\n",
        "This gives rise to some interesting questions:\n",
        "* Machine Learning: How do we deal with this when building a supervised machine   learning model?\n",
        "* Ecology: Which species co-exist with each other?\n",
        "* These are only a small number of examples of the fascinating questions this datasets\n",
        "  allows us to answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SPWX35wdJlf",
        "colab_type": "text"
      },
      "source": [
        "# Training a first model\n",
        "\n",
        "In this section we will build, train & evaluate two simple image recognition models:\n",
        "\n",
        "*   **Using sklearn**: A logistic regression classifier on the ResNet embeddings.\n",
        "*   **Using Tensorflow**: A convolutional Neural Network (CNN) on raw image data.\n",
        "\n",
        "For now, we'll make some simplifying assumptions:\n",
        "\n",
        "* We will only deal with image classification and thus ignore the `count` label.\n",
        "* If multiple photographs are present in a single entry, we will only use the first one.\n",
        "* Similarly, if multiple species labels are available, we only select the first label in the list.\n",
        "* For the CNN model, we downscale images to a size of [150, 150], using tensorflow's `tf.image.resize`\n",
        "\n",
        "During this week, you should rethink the process of association between an image and its labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCG_h2wiDI2",
        "colab_type": "text"
      },
      "source": [
        "#### Helper functions for reading the TFRecord files\n",
        "\n",
        "So now, we know how to read and display an image from tfrecord files using TFRecorder 2.0.\n",
        "\n",
        "\n",
        "Let's define some functions to get the data and associated labels for training.\n",
        "\n",
        "Needed features:\n",
        "\n",
        "\n",
        "1.   Data: \"**Images**\" & \"**Embeddings**\"\n",
        "2.   Labels: \"**Count**\" and \"**Species**\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCyISlAEidlC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Adding some parsing & preprocessing utility functions\n",
        "\n",
        "feature_description = { \n",
        "    'Images': tf.compat.v1.FixedLenSequenceFeature(\n",
        "        [], tf.string, allow_missing=True),\n",
        "    'Embeddings': tf.compat.v1.FixedLenSequenceFeature(\n",
        "        [], tf.string, allow_missing=True),\n",
        "    'SiteID': tf.compat.v1.FixedLenFeature([], tf.string, default_value=''),\n",
        "    'CaptureEventID': tf.compat.v1.FixedLenFeature(\n",
        "        [], tf.string, default_value=''),\n",
        "    'LocationX': tf.compat.v1.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    'LocationY': tf.compat.v1.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    'DateTime': tf.compat.v1.FixedLenFeature([], tf.string, default_value=''),\n",
        "    # Labels\n",
        "    'NumSpecies': tf.compat.v1.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    'Count': tf.compat.v1.VarLenFeature(tf.string),\n",
        "    'Species': tf.compat.v1.VarLenFeature(tf.string),\n",
        "}\n",
        "\n",
        "def deserialize_embeddings(serialized_embeddings):\n",
        "  \"\"\"Deserialize embeddings and set shape.\"\"\"\n",
        "  parse_float32_tensor = functools.partial(\n",
        "      tf.io.parse_tensor, out_type=tf.float32)\n",
        "  embeddings = tf.map_fn(\n",
        "      parse_float32_tensor, serialized_embeddings, dtype=tf.float32) \n",
        "  # `None` indicates a dimension of varying size, 128 is the size of the feature vector\n",
        "  embeddings.set_shape([None, 1, 128])\n",
        "\n",
        "  return embeddings[:, 0, :]\n",
        "\n",
        "def parse_function(example_proto):\n",
        "  \"\"\"Parse the input tf.Example proto using the dictionary above.\"\"\"\n",
        "  \n",
        "  parsed_example = tf.compat.v1.parse_single_example(\n",
        "      example_proto, feature_description)\n",
        "  \n",
        "  # Decode jpeg encoded image data\n",
        "  parsed_example[\"Images\"] = tf.map_fn(\n",
        "      tf.image.decode_jpeg, parsed_example[\"Images\"], dtype=tf.uint8) \n",
        "  \n",
        "  # Deserialize embeddings\n",
        "  parsed_example[\"Embeddings\"] = deserialize_embeddings(\n",
        "      parsed_example[\"Embeddings\"])\n",
        "  \n",
        "  # Convert sparse string tensor to dense\n",
        "  parsed_example[\"Species\"] = tf.compat.v1.sparse_tensor_to_dense(\n",
        "      parsed_example[\"Species\"], default_value=\"none\", name=\"Species\")\n",
        "  parsed_example[\"Count\"] = tf.compat.v1.sparse_tensor_to_dense(\n",
        "      parsed_example[\"Count\"], default_value='0', name='Count')\n",
        "  \n",
        "  return parsed_example\n",
        "\n",
        "\n",
        "def image_preprocessing(image, downsized_image_size):\n",
        "  \"\"\"Some basic image preprocessing.\"\"\"\n",
        "  image = tf.cast(image, tf.float32)\n",
        "\n",
        "  # Re-scale features to be in [0, 1]\n",
        "  image = (image / 255.0) \n",
        "  image = tf.image.resize(\n",
        "      image, (downsized_image_size, downsized_image_size))\n",
        "  \n",
        "  return image\n",
        "\n",
        "\n",
        "def format_data(example, downsized_image_size=150):\n",
        "  \"\"\"Access a subset of available features and labels.\"\"\"\n",
        "  \n",
        "  # Only deal with the first image, embedding & label\n",
        "  image = image_preprocessing(example['Images'][0], downsized_image_size)\n",
        "  label = example['Species'][0]\n",
        "  embedding = example['Embeddings'][0]\n",
        "  \n",
        "  return image, embedding, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syZ0UtrMA5WH",
        "colab_type": "text"
      },
      "source": [
        "## Trying a linear model on the ResNet embeddings (with sklearn)\n",
        "\n",
        "When facing a new machine learning problem, it is often useful to try a simple algorithm first. Since we've been provided with ResNet features, we could simply try a linear classifier on those features. \n",
        "\n",
        "Note that sklearn's `fit` method expects numpy arrays. In order to provide this, we will convert from tensorflow tensors to numpy arrays using the function `load_data_as_np_arrays`. **WARNING:** All the data returned from this function will be kept in RAM. Be aware of this when attempting to use the entire training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shyUAQgDZLuZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Convert data to numpy arrays\n",
        "\n",
        "def load_data_as_np_arrays(dataset, return_images=False):\n",
        "  \"\"\"Load data as numpy arrays. Optionally return raw image data\"\"\"\n",
        "  images = [] if return_images else None\n",
        "  embeddings = []\n",
        "  labels = []\n",
        "  \n",
        "  for i, e, l in dataset:\n",
        "    if return_images:\n",
        "      images.append(i.numpy())\n",
        "    embeddings.append(e.numpy())\n",
        "    labels.append(_SPECIES_TO_CLASS_ID[l.numpy().decode('utf-8')])\n",
        "  \n",
        "  # Convert to numpy array\n",
        "  if return_images:\n",
        "    images = np.array(images)\n",
        "  embeddings = np.array(embeddings)\n",
        "  labels = np.array(labels)\n",
        "  \n",
        "  return images, embeddings, labels\n",
        "\n",
        "n_train_images_to_use = 5000 #@param\n",
        "\n",
        "train_set = training_dataset.map(parse_function).map(format_data).take(n_train_images_to_use)\n",
        "# Use the entire validation and test set\n",
        "valid_set = validation_dataset.map(parse_function).map(format_data)\n",
        "test_set = test_dataset.map(parse_function).map(format_data)\n",
        "\n",
        "# Load data as numpy arrays\n",
        "_, train_embeddings, train_labels = load_data_as_np_arrays(train_set)\n",
        "_, valid_embeddings, valid_labels = load_data_as_np_arrays(valid_set)\n",
        "test_images, test_embeddings, test_labels = load_data_as_np_arrays(test_set, return_images=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgBha-BU9WQq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Training Logistic Regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "max_training_iterations = 5000 #@param\n",
        "\n",
        "\n",
        "# Set up a LogisticRegression object\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
        "                         multi_class='multinomial', \n",
        "                         max_iter=max_training_iterations)\n",
        "# Train on training data\n",
        "clf.fit(train_embeddings, train_labels)\n",
        "\n",
        "valid_predictions = clf.predict(valid_embeddings)\n",
        "valid_acc = (valid_predictions == valid_labels).mean()\n",
        "\n",
        "print(\"Your validation accuracy is: {:.2f}%\".format(100 * valid_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BdhcB6syrjL",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "We now use our trained model to classify a sample of 25 images from the test set. We pass these 25 images to the  ```model.predict``` function, which returns a [25, 10] dimensional matrix. The entry at position $(i, j)$ of this matrix contains the probability that image $i$ belongs to class $j$. We obtain the most-likely prediction using the ```np.argmax``` function which returns the index of the maximum entry along the columns. Finally, we plot the result with the prediction and prediction probability labelled underneath the image and true label on the side. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZv-vhV_972F",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test set results\n",
        "\n",
        "test_predictions = clf.predict(test_embeddings)\n",
        "test_acc = (test_predictions == test_labels).mean()\n",
        "\n",
        "print(\"Your test accuracy is: {:.2f}%\".format(100 * test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjzP384wm9OW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Example test set classifications\n",
        "\n",
        "n_test_examples = 25 #@param \n",
        "\n",
        "img_indices = np.random.randint(0, len(test_images), size=[n_test_examples])\n",
        "sample_test_images = test_images[img_indices]\n",
        "sample_test_embeddings = test_embeddings[img_indices]\n",
        "sample_test_labels = [_SPECIES[i] for i in test_labels[img_indices].squeeze()]\n",
        "\n",
        "predictions = clf.predict_proba(sample_test_embeddings)\n",
        "max_prediction = np.argmax(predictions, axis=1)\n",
        "prediction_probs = np.max(predictions, axis=1)\n",
        "\n",
        "plt.figure(figsize=(17,17))\n",
        "for i, (img, prediction, prob, true_label) in enumerate(\n",
        "    zip(sample_test_images, max_prediction, prediction_probs, sample_test_labels)):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "\n",
        "  plt.imshow(img)\n",
        "  plt.xlabel('{} ({:0.3f})'.format(_SPECIES[prediction], prob))\n",
        "  plt.ylabel('{}'.format(true_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4DJ2ixxfYp4",
        "colab_type": "text"
      },
      "source": [
        "## Training first convolutional neural network (CNN)\n",
        "\n",
        "For those of you keen on putting your Deep Learning skills into practice, we will now train a first CNN on raw image data. Because we can do everything in tensorflow, we won't need to manually convert everything to numpy first. This also means we can easily use the entire training dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vylCvpsI9EZA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Setting up the model. From the CNN practical here: https://colab.research.google.com/drive/1GhO1DN8J1lmgIgV1zuKKMWd6m0SCUC5m\n",
        "\n",
        "# Model parameters\n",
        "n_filters_1 = 48 #@param\n",
        "n_filters_2 = 128 #@param\n",
        "n_filters_3 = 192 #@param\n",
        "n_filters_4 = 192 #@param\n",
        "n_filters_5 = 128 #@param\n",
        "n_units_dense_1 = 1024 #@param\n",
        "\n",
        "\n",
        "conv2d_3x3_relu = functools.partial(tf.keras.layers.Conv2D,\n",
        "                                    kernel_size=(3,3),\n",
        "                                    activation=tf.nn.relu,\n",
        "                                    padding='same')\n",
        "\n",
        "\n",
        "cnn_model = tf.keras.models.Sequential([\n",
        "    conv2d_3x3_relu(filters=n_filters_1),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
        "    conv2d_3x3_relu(filters=n_filters_2),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
        "    conv2d_3x3_relu(filters=n_filters_3),\n",
        "    conv2d_3x3_relu(filters=n_filters_4),\n",
        "    conv2d_3x3_relu(filters=n_filters_5),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(3,3)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(n_units_dense_1, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(len(_SPECIES)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zuii0Omq1jbI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Setting up data, losses, optimizer and the training step\n",
        "  \n",
        "learning_rate = 1e-4 #@param\n",
        "batch_size = 32 #@param\n",
        "n_training_examples_to_use = 1000 #@param\n",
        "\n",
        "# Set up a loss and an optimizer\n",
        "loss_fn = tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# Metrics \n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "eval_loss = tf.keras.metrics.Mean(name='eval_loss')\n",
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='eval_accuracy')\n",
        "\n",
        "# Prepare training and validation datasets\n",
        "training_dataset_cnn = training_dataset.map(parse_function).map(format_data).take(n_training_examples_to_use).batch(batch_size)\n",
        "validation_dataset_cnn = validation_dataset.map(parse_function).map(format_data).batch(256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2kg0zmHDI6",
        "colab_type": "code",
        "outputId": "94f60e89-3325-4a25-f759-5baba646512b",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1 #@param\n",
        "report_every = 25 #@param\n",
        "\n",
        "\n",
        "def _labels_to_ids(labels):\n",
        "  return [_SPECIES_TO_CLASS_ID[l.decode('utf-8')] \n",
        "          for l in labels.numpy()]\n",
        "\n",
        "\n",
        "def _evaluate_model(model, dataset):\n",
        "  # Evaluation loop\n",
        "  eval_acc = []\n",
        "  n_examples = 0\n",
        "  for eval_images, _, eval_labels in dataset:\n",
        "    eval_labels = _labels_to_ids(eval_labels)\n",
        "    eval_predictions = tf.argmax(model(eval_images), axis=1)\n",
        "    \n",
        "    n_examples += len(eval_labels)\n",
        "    eval_acc.append((eval_predictions.numpy() == eval_labels).sum())\n",
        "\n",
        "  return np.sum(eval_acc) / n_examples\n",
        "  \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  # Training loop\n",
        "  idx = 0\n",
        "  for images, _, labels in training_dataset_cnn:\n",
        "    labels = _labels_to_ids(labels)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = cnn_model(images)\n",
        "      loss = tf.reduce_mean(loss_fn(labels=labels, logits=logits))\n",
        "      \n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    \n",
        "    # Backpropagation\n",
        "    gradients = tape.gradient(loss, cnn_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, cnn_model.trainable_variables))      \n",
        "     \n",
        "    if 0 == (idx % report_every):\n",
        "      training_acc = (predictions.numpy() == labels).mean()\n",
        "      print('Step {}: Training loss: {:.4f} Training accuracy: {:.2f}'.format(\n",
        "            idx, loss.numpy(), training_acc))\n",
        "    idx += 1\n",
        "    \n",
        "  valid_acc = _evaluate_model(cnn_model, validation_dataset_cnn)\n",
        "  print(\"Your validation accuracy is: {:.2f}%\".format(100 * valid_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 2525: Training loss: 1.4372 Training accuracy: 0.59\n",
            "Step 2550: Training loss: 1.1766 Training accuracy: 0.62\n",
            "Step 2575: Training loss: 0.9570 Training accuracy: 0.56\n",
            "Step 2600: Training loss: 0.6878 Training accuracy: 0.81\n",
            "Step 2625: Training loss: 0.8653 Training accuracy: 0.62\n",
            "Step 2650: Training loss: 1.1398 Training accuracy: 0.53\n",
            "Step 2675: Training loss: 0.8250 Training accuracy: 0.66\n",
            "Step 2700: Training loss: 0.4986 Training accuracy: 0.81\n",
            "Step 2725: Training loss: 0.8651 Training accuracy: 0.69\n",
            "Step 2750: Training loss: 0.8714 Training accuracy: 0.66\n",
            "Step 2775: Training loss: 0.9167 Training accuracy: 0.66\n",
            "Step 2800: Training loss: 0.7035 Training accuracy: 0.78\n",
            "Step 2825: Training loss: 1.2792 Training accuracy: 0.62\n",
            "Step 2850: Training loss: 0.7508 Training accuracy: 0.69\n",
            "Step 2875: Training loss: 1.2466 Training accuracy: 0.69\n",
            "Step 2900: Training loss: 0.5882 Training accuracy: 0.81\n",
            "Step 2925: Training loss: 0.7674 Training accuracy: 0.78\n",
            "Step 2950: Training loss: 0.7651 Training accuracy: 0.75\n",
            "Step 2975: Training loss: 0.4795 Training accuracy: 0.81\n",
            "Step 3000: Training loss: 0.4987 Training accuracy: 0.78\n",
            "Step 3025: Training loss: 0.4209 Training accuracy: 0.91\n",
            "Step 3050: Training loss: 0.5308 Training accuracy: 0.91\n",
            "Step 3075: Training loss: 0.9437 Training accuracy: 0.62\n",
            "Step 3100: Training loss: 0.8533 Training accuracy: 0.75\n",
            "Step 3125: Training loss: 0.6004 Training accuracy: 0.78\n",
            "Step 3150: Training loss: 0.6634 Training accuracy: 0.69\n",
            "Step 3175: Training loss: 0.5665 Training accuracy: 0.84\n",
            "Step 3200: Training loss: 0.9008 Training accuracy: 0.75\n",
            "Step 3225: Training loss: 0.8223 Training accuracy: 0.81\n",
            "Step 3250: Training loss: 0.7826 Training accuracy: 0.81\n",
            "Step 3275: Training loss: 0.6634 Training accuracy: 0.78\n",
            "Step 3300: Training loss: 0.7722 Training accuracy: 0.75\n",
            "Step 3325: Training loss: 0.9704 Training accuracy: 0.69\n",
            "Step 3350: Training loss: 0.9690 Training accuracy: 0.72\n",
            "Step 3375: Training loss: 0.5837 Training accuracy: 0.78\n",
            "Step 3400: Training loss: 0.8257 Training accuracy: 0.72\n",
            "Step 3425: Training loss: 0.9185 Training accuracy: 0.72\n",
            "Step 3450: Training loss: 0.7071 Training accuracy: 0.75\n",
            "Step 3475: Training loss: 0.7197 Training accuracy: 0.75\n",
            "Step 3500: Training loss: 0.7198 Training accuracy: 0.81\n",
            "Step 3525: Training loss: 0.6361 Training accuracy: 0.81\n",
            "Step 3550: Training loss: 0.6840 Training accuracy: 0.75\n",
            "Step 3575: Training loss: 0.7233 Training accuracy: 0.72\n",
            "Step 3600: Training loss: 0.8605 Training accuracy: 0.81\n",
            "Step 3625: Training loss: 0.7629 Training accuracy: 0.72\n",
            "Step 3650: Training loss: 0.9138 Training accuracy: 0.81\n",
            "Step 3675: Training loss: 0.6684 Training accuracy: 0.84\n",
            "Step 3700: Training loss: 0.6371 Training accuracy: 0.75\n",
            "Step 3725: Training loss: 1.2753 Training accuracy: 0.66\n",
            "Step 3750: Training loss: 0.5960 Training accuracy: 0.75\n",
            "Step 3775: Training loss: 1.1787 Training accuracy: 0.59\n",
            "Step 3800: Training loss: 0.4162 Training accuracy: 0.88\n",
            "Step 3825: Training loss: 0.9235 Training accuracy: 0.69\n",
            "Step 3850: Training loss: 0.9569 Training accuracy: 0.75\n",
            "Step 3875: Training loss: 0.7809 Training accuracy: 0.69\n",
            "Step 3900: Training loss: 0.8385 Training accuracy: 0.84\n",
            "Step 3925: Training loss: 0.7669 Training accuracy: 0.78\n",
            "Step 3950: Training loss: 0.8747 Training accuracy: 0.62\n",
            "Step 3975: Training loss: 0.7104 Training accuracy: 0.84\n",
            "Step 4000: Training loss: 0.8567 Training accuracy: 0.72\n",
            "Step 4025: Training loss: 0.6799 Training accuracy: 0.78\n",
            "Step 4050: Training loss: 0.5841 Training accuracy: 0.88\n",
            "Step 4075: Training loss: 0.9179 Training accuracy: 0.75\n",
            "Step 4100: Training loss: 0.6643 Training accuracy: 0.69\n",
            "Step 4125: Training loss: 0.7493 Training accuracy: 0.78\n",
            "Step 4150: Training loss: 0.8937 Training accuracy: 0.59\n",
            "Step 4175: Training loss: 1.0763 Training accuracy: 0.66\n",
            "Step 4200: Training loss: 0.9983 Training accuracy: 0.72\n",
            "Step 4225: Training loss: 0.9843 Training accuracy: 0.66\n",
            "Step 4250: Training loss: 0.7015 Training accuracy: 0.75\n",
            "Step 4275: Training loss: 0.4261 Training accuracy: 0.88\n",
            "Step 4300: Training loss: 1.2460 Training accuracy: 0.59\n",
            "Step 4325: Training loss: 0.9831 Training accuracy: 0.66\n",
            "Step 4350: Training loss: 0.8590 Training accuracy: 0.62\n",
            "Step 4375: Training loss: 0.9503 Training accuracy: 0.69\n",
            "Step 4400: Training loss: 1.0053 Training accuracy: 0.66\n",
            "Step 4425: Training loss: 1.0804 Training accuracy: 0.75\n",
            "Step 4450: Training loss: 0.6355 Training accuracy: 0.84\n",
            "Step 4475: Training loss: 0.9188 Training accuracy: 0.59\n",
            "Step 4500: Training loss: 0.4348 Training accuracy: 0.91\n",
            "Step 4525: Training loss: 0.9253 Training accuracy: 0.66\n",
            "Step 4550: Training loss: 0.7240 Training accuracy: 0.72\n",
            "Step 4575: Training loss: 0.5730 Training accuracy: 0.88\n",
            "Step 4600: Training loss: 0.6911 Training accuracy: 0.81\n",
            "Step 4625: Training loss: 1.2068 Training accuracy: 0.66\n",
            "Step 4650: Training loss: 0.9526 Training accuracy: 0.62\n",
            "Step 4675: Training loss: 0.6481 Training accuracy: 0.81\n",
            "Step 4700: Training loss: 0.7970 Training accuracy: 0.75\n",
            "Step 4725: Training loss: 0.8264 Training accuracy: 0.66\n",
            "Step 4750: Training loss: 0.6299 Training accuracy: 0.78\n",
            "Step 4775: Training loss: 0.5223 Training accuracy: 0.84\n",
            "Step 4800: Training loss: 0.7510 Training accuracy: 0.72\n",
            "Step 4825: Training loss: 0.7106 Training accuracy: 0.72\n",
            "Step 4850: Training loss: 0.8125 Training accuracy: 0.72\n",
            "Step 4875: Training loss: 0.9623 Training accuracy: 0.69\n",
            "Step 4900: Training loss: 0.7622 Training accuracy: 0.72\n",
            "Step 4925: Training loss: 0.9285 Training accuracy: 0.75\n",
            "Step 4950: Training loss: 0.6985 Training accuracy: 0.78\n",
            "Step 4975: Training loss: 0.8981 Training accuracy: 0.66\n",
            "Step 5000: Training loss: 0.5061 Training accuracy: 0.84\n",
            "Step 5025: Training loss: 0.8466 Training accuracy: 0.72\n",
            "Step 5050: Training loss: 0.6761 Training accuracy: 0.75\n",
            "Step 5075: Training loss: 0.7470 Training accuracy: 0.78\n",
            "Step 5100: Training loss: 0.5903 Training accuracy: 0.81\n",
            "Step 5125: Training loss: 0.6790 Training accuracy: 0.81\n",
            "Step 5150: Training loss: 0.6843 Training accuracy: 0.75\n",
            "Step 5175: Training loss: 0.7477 Training accuracy: 0.72\n",
            "Step 5200: Training loss: 0.5749 Training accuracy: 0.78\n",
            "Step 5225: Training loss: 0.9700 Training accuracy: 0.66\n",
            "Step 5250: Training loss: 0.8847 Training accuracy: 0.66\n",
            "Step 5275: Training loss: 0.8276 Training accuracy: 0.72\n",
            "Step 5300: Training loss: 1.0730 Training accuracy: 0.66\n",
            "Step 5325: Training loss: 0.8602 Training accuracy: 0.72\n",
            "Step 5350: Training loss: 0.8727 Training accuracy: 0.69\n",
            "Step 5375: Training loss: 1.2016 Training accuracy: 0.69\n",
            "Step 5400: Training loss: 0.9523 Training accuracy: 0.59\n",
            "Step 5425: Training loss: 0.8178 Training accuracy: 0.75\n",
            "Step 5450: Training loss: 0.7009 Training accuracy: 0.84\n",
            "Step 5475: Training loss: 0.9068 Training accuracy: 0.72\n",
            "Step 5500: Training loss: 1.1565 Training accuracy: 0.62\n",
            "Step 5525: Training loss: 0.6067 Training accuracy: 0.78\n",
            "Step 5550: Training loss: 1.1951 Training accuracy: 0.62\n",
            "Step 5575: Training loss: 0.6195 Training accuracy: 0.84\n",
            "Step 5600: Training loss: 1.3344 Training accuracy: 0.56\n",
            "Step 5625: Training loss: 0.8729 Training accuracy: 0.62\n",
            "Step 5650: Training loss: 0.7255 Training accuracy: 0.78\n",
            "Step 5675: Training loss: 0.9725 Training accuracy: 0.69\n",
            "Step 5700: Training loss: 0.9424 Training accuracy: 0.72\n",
            "Step 5725: Training loss: 0.9199 Training accuracy: 0.75\n",
            "Step 5750: Training loss: 0.8175 Training accuracy: 0.75\n",
            "Step 5775: Training loss: 0.9578 Training accuracy: 0.75\n",
            "Step 5800: Training loss: 0.4685 Training accuracy: 0.94\n",
            "Step 5825: Training loss: 0.9900 Training accuracy: 0.59\n",
            "Step 5850: Training loss: 0.9644 Training accuracy: 0.72\n",
            "Step 5875: Training loss: 0.9166 Training accuracy: 0.75\n",
            "Step 5900: Training loss: 0.8854 Training accuracy: 0.66\n",
            "Step 5925: Training loss: 0.8611 Training accuracy: 0.81\n",
            "Step 5950: Training loss: 1.0588 Training accuracy: 0.75\n",
            "Step 5975: Training loss: 0.9490 Training accuracy: 0.72\n",
            "Step 6000: Training loss: 0.7415 Training accuracy: 0.69\n",
            "Step 6025: Training loss: 0.7834 Training accuracy: 0.69\n",
            "Step 6050: Training loss: 0.7544 Training accuracy: 0.81\n",
            "Step 6075: Training loss: 0.5927 Training accuracy: 0.81\n",
            "Step 6100: Training loss: 0.7510 Training accuracy: 0.84\n",
            "Step 6125: Training loss: 0.7692 Training accuracy: 0.81\n",
            "Step 6150: Training loss: 0.6917 Training accuracy: 0.78\n",
            "Step 6175: Training loss: 0.6497 Training accuracy: 0.81\n",
            "Step 6200: Training loss: 0.8668 Training accuracy: 0.66\n",
            "Step 6225: Training loss: 0.9843 Training accuracy: 0.72\n",
            "Step 6250: Training loss: 1.4739 Training accuracy: 0.62\n",
            "Step 6275: Training loss: 0.8326 Training accuracy: 0.75\n",
            "Step 6300: Training loss: 1.2092 Training accuracy: 0.69\n",
            "Step 6325: Training loss: 0.8354 Training accuracy: 0.75\n",
            "Step 6350: Training loss: 1.0086 Training accuracy: 0.69\n",
            "Step 6375: Training loss: 0.9743 Training accuracy: 0.66\n",
            "Step 6400: Training loss: 0.6627 Training accuracy: 0.78\n",
            "Step 6425: Training loss: 0.8574 Training accuracy: 0.69\n",
            "Step 6450: Training loss: 0.9243 Training accuracy: 0.66\n",
            "Step 6475: Training loss: 0.4884 Training accuracy: 0.88\n",
            "Step 6500: Training loss: 0.4821 Training accuracy: 0.81\n",
            "Step 6525: Training loss: 0.8545 Training accuracy: 0.72\n",
            "Step 6550: Training loss: 0.7878 Training accuracy: 0.72\n",
            "Step 6575: Training loss: 0.8217 Training accuracy: 0.69\n",
            "Step 6600: Training loss: 0.8410 Training accuracy: 0.66\n",
            "Step 6625: Training loss: 1.0757 Training accuracy: 0.56\n",
            "Step 6650: Training loss: 0.6143 Training accuracy: 0.81\n",
            "Step 6675: Training loss: 0.9870 Training accuracy: 0.66\n",
            "Step 6700: Training loss: 1.2391 Training accuracy: 0.59\n",
            "Step 6725: Training loss: 1.1205 Training accuracy: 0.69\n",
            "Step 6750: Training loss: 0.8149 Training accuracy: 0.72\n",
            "Step 6775: Training loss: 1.3639 Training accuracy: 0.56\n",
            "Step 6800: Training loss: 1.2920 Training accuracy: 0.66\n",
            "Step 6825: Training loss: 0.6800 Training accuracy: 0.72\n",
            "Step 6850: Training loss: 1.2370 Training accuracy: 0.56\n",
            "Step 6875: Training loss: 1.8012 Training accuracy: 0.47\n",
            "Step 6900: Training loss: 1.1952 Training accuracy: 0.53\n",
            "Step 6925: Training loss: 1.7164 Training accuracy: 0.44\n",
            "Step 6950: Training loss: 0.6531 Training accuracy: 0.78\n",
            "Step 6975: Training loss: 1.1661 Training accuracy: 0.62\n",
            "Step 7000: Training loss: 0.9653 Training accuracy: 0.66\n",
            "Step 7025: Training loss: 0.6960 Training accuracy: 0.72\n",
            "Step 7050: Training loss: 0.6891 Training accuracy: 0.78\n",
            "Step 7075: Training loss: 0.7631 Training accuracy: 0.78\n",
            "Step 7100: Training loss: 0.6377 Training accuracy: 0.81\n",
            "Step 7125: Training loss: 0.7647 Training accuracy: 0.72\n",
            "Step 7150: Training loss: 0.8805 Training accuracy: 0.69\n",
            "Step 7175: Training loss: 1.0206 Training accuracy: 0.69\n",
            "Step 7200: Training loss: 0.6202 Training accuracy: 0.78\n",
            "Step 7225: Training loss: 1.0154 Training accuracy: 0.56\n",
            "Step 7250: Training loss: 0.5907 Training accuracy: 0.75\n",
            "Step 7275: Training loss: 0.8108 Training accuracy: 0.66\n",
            "Step 7300: Training loss: 0.6489 Training accuracy: 0.72\n",
            "Step 7325: Training loss: 0.5495 Training accuracy: 0.88\n",
            "Step 7350: Training loss: 0.8716 Training accuracy: 0.72\n",
            "Step 7375: Training loss: 0.6368 Training accuracy: 0.75\n",
            "13157\n",
            "Your validation accuracy is: 63.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlrSyMQpoV9f",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq5deOk4cMSd",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "7bfba6b7-a574-4622-d6a8-76d5ea747e33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#@title Load test data\n",
        "\n",
        "test_dataset_cnn = test_dataset.map(parse_function).map(format_data).batch(256)\n",
        "test_acc = _evaluate_model(cnn_model, test_dataset_cnn)\n",
        "print(\"Your test accuracy is: {:.2f}%\".format(100 * test_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13154\n",
            "Your test accuracy is: 62.79%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}